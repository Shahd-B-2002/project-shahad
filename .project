import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score

# 1. Load the data
df = pd.read_csv("heart.csv")

# 2. Drop irrelevant columns
df = df.drop(columns=["id", "dataset"])

# 3. One-hot encode all categorical columns
df = pd.get_dummies(df, drop_first=True)

# 4. Drop any remaining missing values
df = df.dropna()

# 5. Separate features and target
X = df.drop("num", axis=1)
y = df["num"]

# 6. Create folders for train/test splits and results
os.makedirs("Data/Train_Test", exist_ok=True)
os.makedirs("Data/Result", exist_ok=True)

# 7. Split into train/test (80/20) with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 8. Save the splits
X_train.to_csv("Data/Train_Test/X_train.csv", index=False)
y_train.to_frame(name="num").to_csv("Data/Train_Test/y_train.csv", index=False)
X_test.to_csv("Data/Train_Test/X_test.csv", index=False)
y_test.to_frame(name="num").to_csv("Data/Train_Test/y_test.csv", index=False)

# 9. Balance the training set with SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train, y_train)

# 10. Standardize features
scaler = StandardScaler()
X_res_scaled = scaler.fit_transform(X_res)
X_test_scaled = scaler.transform(X_test)

# 11. Define the models
models = {
    "LogisticRegression": LogisticRegression(max_iter=1000, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "DecisionTree": DecisionTreeClassifier(random_state=42),
    "SVM": SVC(random_state=42),
    "NaiveBayes": GaussianNB(),
    "RandomForest": RandomForestClassifier(n_estimators=100, random_state=42)
}

# 12. Train, predict, save results, and collect accuracies
results = []
for name, clf in models.items():
    clf.fit(X_res_scaled, y_res)
    y_pred = clf.predict(X_test_scaled)
    # Save predictions
    pd.DataFrame({"num_pred": y_pred}).to_csv(f"Data/Result/predictions_{name}.csv", index=False)
    # Record accuracy
    acc = accuracy_score(y_test, y_pred)
    results.append((name, acc))

# 13. Create a DataFrame of results and plot
results_df = pd.DataFrame(results, columns=["Model", "Accuracy"])
print(results_df)

plt.figure(figsize=(10,6))
sns.barplot(x="Model", y="Accuracy", data=results_df, palette="viridis")
plt.ylim(0, 1)
plt.title("Model Accuracies")
plt.ylabel("Accuracy")
plt.xlabel("Model")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
